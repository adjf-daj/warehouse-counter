#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä»“åº“è´§ç‰©æ£€æµ‹ä¸“ä¸šç‰ˆ Web App (V9 Fixed)
ä¿®å¤ IoU æ»‘å—å¯¹å…¨å±€å»é‡å¤±æ•ˆçš„ BUG
"""

import streamlit as st
import cv2
from ultralytics import YOLO
import os
import numpy as np
import tempfile
import random
from PIL import Image
import shutil
import time

# è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(
    page_title="AI ä»“åº“è§†è§‰ç›˜ç‚¹ Pro",
    page_icon="ğŸ­",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ==================== åç«¯é€»è¾‘ ====================

@st.cache_resource(show_spinner=False)
def load_model():
    """ç¼“å­˜åŠ è½½æ¨¡å‹"""
    MODEL_PATH = 'yolov8l-world.pt'
    CLASSES = [
        'textile bale', 'woven sack', 'pillow', 'sandbag',
        'wrapped package', 'stacked white sacks', 'wall of bales'
    ]
    try:
        model = YOLO(MODEL_PATH)
        model.set_classes(CLASSES)
        return model
    except Exception as e:
        st.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
        return None

def detect_warehouse_goods_v7_web(image_path, conf, iou, model):
    """V7 æ ¸å¿ƒæ£€æµ‹é€»è¾‘ (Webé€‚é…ç‰ˆ)"""
    # å‚æ•°é…ç½®
    MIN_AREA_RATIO = 0.001
    SLICE_HEIGHT, SLICE_WIDTH = 640, 640
    SLICE_OVERLAP = 0.2
    AGNOSTIC_NMS = True
    
    # [å…³é”®ä¿®å¤] è®©å…¨å±€å»é‡é˜ˆå€¼ç›´æ¥ç­‰äºç”¨æˆ·è®¾ç½®çš„ IoU
    DEDUP_THRESHOLD = iou 

    original_img = cv2.imread(image_path)
    if original_img is None: return None
    h, w = original_img.shape[:2]
    min_area = w * h * MIN_AREA_RATIO

    # åˆ‡ç‰‡è®¡ç®—
    overlap_h, overlap_w = int(SLICE_HEIGHT * SLICE_OVERLAP), int(SLICE_WIDTH * SLICE_OVERLAP)
    slices = []
    y_start = 0
    while y_start < h:
        y_end = min(y_start + SLICE_HEIGHT, h)
        x_start = 0
        while x_start < w:
            x_end = min(x_start + SLICE_WIDTH, w)
            x1, y1 = max(0, x_start - overlap_w if x_start > 0 else 0), max(0, y_start - overlap_h if y_start > 0 else 0)
            x2, y2 = min(w, x_end + overlap_w if x_end < w else w), min(h, y_end + overlap_h if y_end < h else h)
            slices.append((x1, y1, x2, y2, x_start, y_start))
            x_start += SLICE_WIDTH - overlap_w
        y_start += SLICE_HEIGHT - overlap_h

    # åˆ‡ç‰‡æ£€æµ‹
    all_boxes = []
    temp_dir = tempfile.mkdtemp()
    progress_bar = st.progress(0)

    try:
        for i, (x1, y1, x2, y2, _, _) in enumerate(slices):
            progress_bar.progress((i + 1) / len(slices), text=f"æ­£åœ¨åˆ†æåˆ‡ç‰‡ {i+1}/{len(slices)}...")
            
            slice_img = original_img[y1:y2, x1:x2]
            temp_path = os.path.join(temp_dir, f"slice_{i}.jpg")
            cv2.imwrite(temp_path, slice_img)
            
            # è¿™é‡Œæ˜¯å±€éƒ¨ NMS
            results = model.predict(source=temp_path, conf=conf, iou=iou, agnostic_nms=AGNOSTIC_NMS, verbose=False)
            
            for box in results[0].boxes:
                xyxy = box.xyxy[0].cpu().numpy()
                # åæ ‡æ˜ å°„å›åŸå›¾
                xyxy[0] += x1; xyxy[1] += y1; xyxy[2] += x1; xyxy[3] += y1
                all_boxes.append({
                    'cls': int(box.cls[0]), 'conf': float(box.conf[0]),
                    'xyxy': xyxy, 'area': (xyxy[2]-xyxy[0])*(xyxy[3]-xyxy[1])
                })
    finally:
        shutil.rmtree(temp_dir)
        progress_bar.empty()

    # --- å…¨å±€ NMS (å…³é”®æ­¥éª¤) ---
    # å…ˆæŒ‰ç½®ä¿¡åº¦æ’åº
    all_boxes.sort(key=lambda x: x['conf'], reverse=True)
    unique_boxes = []
    
    for box in all_boxes:
        is_duplicate = False
        # æ‹¿å½“å‰æ¡†å»å’Œå·²ç»ä¿ç•™çš„æ¡†åšå¯¹æ¯”
        for xb in unique_boxes:
            # å¦‚æœé‡å åº¦è¶…è¿‡äº†ç”¨æˆ·è®¾å®šçš„ DEDUP_THRESHOLD (ä¾‹å¦‚ 0.1)
            if compute_iou(box['xyxy'], xb['xyxy']) > DEDUP_THRESHOLD:
                is_duplicate = True
                break # åªè¦å’Œä¸€ä¸ªé‡å è¿‡é«˜ï¼Œå°±ä¸¢å¼ƒ
        
        if not is_duplicate:
            unique_boxes.append(box)

    # å°ºå¯¸è¿‡æ»¤
    final_boxes = [b for b in unique_boxes if b['area'] >= min_area]

    # å¯è§†åŒ–
    annotated_img = original_img.copy()
    random.seed(42)
    colors = {i: (random.randint(50, 255), random.randint(50, 255), random.randint(50, 255)) for i in range(len(model.names))}
    class_counts = {}
    
    for box in final_boxes:
        cls_id = box['cls']
        class_name = model.names[cls_id]
        class_counts[class_name] = class_counts.get(class_name, 0) + 1
        x1, y1, x2, y2 = map(int, box['xyxy'])
        cv2.rectangle(annotated_img, (x1, y1), (x2, y2), colors[cls_id], 2)

    annotated_img_rgb = cv2.cvtColor(annotated_img, cv2.COLOR_BGR2RGB)

    return {
        'final_count': len(final_boxes),
        'raw_count': len(unique_boxes), # è¿™é‡Œå…¶å®å·²ç»æ˜¯NMSåçš„äº†ï¼Œä¸ºäº†ä¸æ··æ·†æ˜¾ç¤º
        'counts_detail': class_counts,
        'result_img_rgb': annotated_img_rgb
    }

def compute_iou(box1, box2):
    """è®¡ç®—ä¸¤ä¸ªæ¡†çš„ IoU (é‡å åº¦)"""
    ix1, iy1 = max(box1[0], box2[0]), max(box1[1], box2[1])
    ix2, iy2 = min(box1[2], box2[2]), min(box1[3], box2[3])
    if ix1 >= ix2 or iy1 >= iy2: return 0.0
    intersection = (ix2 - ix1) * (iy2 - iy1)
    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])
    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])
    return intersection / (area1 + area2 - intersection + 1e-6)

# ==================== å‰ç«¯ UI ====================

def main():
    with st.spinner("ğŸ­ æ­£åœ¨åˆå§‹åŒ– AI å¼•æ“..."):
        model = load_model()

    if model is None: st.stop()

    with st.sidebar:
        st.title("âš™ï¸ æ§åˆ¶é¢æ¿")
        st.subheader("å‚æ•°å¾®è°ƒ")
        # é»˜è®¤å€¼è®¾ä¸º 0.2ï¼Œæ–¹ä¾¿ä½ ç›´æ¥æµ‹è¯•
        conf_val = st.slider("ç½®ä¿¡åº¦ (Conf)", 0.01, 0.5, 0.15, help="è¿‡æ»¤æ‰å¾—åˆ†ä½çš„æ¡†")
        iou_val = st.slider("å»é‡é˜ˆå€¼ (IoU)", 0.05, 0.8, 0.2, help="è¶Šå°å»é‡è¶Šç‹ ã€‚è®¾ä¸º0.1è¡¨ç¤ºåªè¦é‡å 10%å°±åˆå¹¶ã€‚")
        st.caption("Version: V9 Fixed")

    st.title("ğŸ­ AI ä»“åº“è§†è§‰ç›˜ç‚¹ Pro")
    
    uploaded_file = st.file_uploader("ğŸ“¤ ä¸Šä¼ ç…§ç‰‡", type=['jpg', 'png'])

    if uploaded_file:
        with tempfile.NamedTemporaryFile(suffix='.jpg', delete=False) as tmp_input:
            input_path = tmp_input.name
            uploaded_file.seek(0)
            tmp_input.write(uploaded_file.read())

        # åªè¦å‚æ•°å˜äº†ï¼Œæˆ–è€…å›¾ç‰‡å˜äº†ï¼Œå°±é‡æ–°è¿è¡Œ
        trigger = f"{uploaded_file.name}_{conf_val}_{iou_val}"
        
        if 'last_trigger' not in st.session_state or st.session_state['last_trigger'] != trigger:
             with st.status("ğŸš€ æ­£åœ¨åˆ†æ (åº”ç”¨æ–°å‚æ•°)...", expanded=True) as status:
                result_data = detect_warehouse_goods_v7_web(input_path, conf_val, iou_val, model)
                if result_data:
                    st.session_state['result_data'] = result_data
                    st.session_state['last_trigger'] = trigger
                    status.update(label="âœ… åˆ†æå®Œæˆ", state="complete", expanded=False)
        
        if 'result_data' in st.session_state:
            data = st.session_state['result_data']
            
            st.subheader("ğŸ“Š åˆ†æçœ‹æ¿")
            col1, col2, col3 = st.columns(3)
            col1.metric("ğŸ“¦ æœ€ç»ˆè®¡æ•°", f"{data['final_count']} ä¸ª")
            col2.metric("ğŸ¯ å‚æ•°çŠ¶æ€", f"IoU={iou_val}")
            
            st.image(data['result_img_rgb'], caption="è¯†åˆ«ç»“æœ", use_container_width=True)
            
            # ä¸‹è½½é€»è¾‘
            img_bgr = cv2.cvtColor(data['result_img_rgb'], cv2.COLOR_RGB2BGR)
            is_success, buffer = cv2.imencode(".jpg", img_bgr)
            st.download_button("ğŸ“¥ ä¸‹è½½ç»“æœå›¾", buffer.tobytes(), "result.jpg", "image/jpeg")

if __name__ == "__main__":
    main()
